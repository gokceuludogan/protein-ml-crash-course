{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokceuludogan/protein-ml-crash-course/blob/wip/Chapter_4_Secondary_Structure_Prediction_with_LSTM_model_%26_Pytorch_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Dw32R0bDUv-"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this chapter, we will predict **protein secondary structure**, which refers to the local folding of the protein's backbone into common motifs such as **alpha helices**, **beta sheets**, and **random coils**. This is a classic problem in bioinformatics that can be addressed using machine learning (ML) or deep learning (DL) techniques.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Understand the protein secondary structure prediction task.\n",
        "- Encode protein sequences and their corresponding secondary structure labels.\n",
        "- Use DL techniques, specifically Recurrent Neural Networks, to build predictive models.\n",
        "- Evaluate the performance of these models.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Protein Secondary Structure\n",
        "\n",
        "### Definitions:\n",
        "\n",
        "- **Primary structure**: The linear sequence of amino acids in the protein.\n",
        "- **Secondary structure**: Local folding patterns of the protein chain, mainly forming alpha helices (H), beta sheets (E), or random coils (C).\n",
        "- **Tertiary structure**: The overall 3D structure of the protein.\n",
        "\n",
        "### Common Classes in Secondary Structure Prediction:\n",
        "\n",
        "- **H**: Alpha-helix\n",
        "- **E**: Beta-sheet\n",
        "- **C**: Coil (random structure)\n",
        "\n",
        "### Dataset Example:\n",
        "\n",
        "We will work with a dataset where each protein sequence is accompanied by its secondary structure labels.\n",
        "\n",
        "| Sequence | Secondary Structure |\n",
        "| --- | --- |\n",
        "| MAGWELV | HCCCCCC |\n",
        "| GGQVNLL | CCECCEE |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Data Preprocessing\n",
        "\n",
        "For this task, we will use the UniProt SwissProt database, which contains manually curated protein sequences. We will download the sequences in FASTA format and compute their secondary structures using DSSP (Define Secondary Structure of Proteins), a tool that assigns secondary structure elements (alpha-helix, beta-sheet, coil) based on 3D coordinates of the protein's structure.\n",
        "\n",
        "### Download UniProt SwissProt Database\n",
        "\n",
        "We will download the SwissProt database from the UniProt FTP server. This database contains high-quality, manually reviewed protein sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6IA8kOqK8Kr",
        "outputId": "ed8f5ad6-3436-4e1b-d90a-bbbad8527585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-04 16:05:50--  https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
            "Resolving ftp.uniprot.org (ftp.uniprot.org)... 128.175.240.195\n",
            "Connecting to ftp.uniprot.org (ftp.uniprot.org)|128.175.240.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92678508 (88M) [application/x-gzip]\n",
            "Saving to: ‘uniprot_sprot.fasta.gz.1’\n",
            "\n",
            "uniprot_sprot.fasta  23%[===>                ]  20,62M  3,85MB/s    eta 20s    "
          ]
        }
      ],
      "source": [
        "!wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
        "!gunzip uniprot_sprot.fasta.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbQqgZ0ZLVVc",
        "outputId": "5db92c06-ce4e-4829-9437-d32bab971d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in ./ccourse/lib/python3.9/site-packages (1.84)\n",
            "Requirement already satisfied: numpy in ./ccourse/lib/python3.9/site-packages (from biopython) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0IKZvsdLQKZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Set the file path for SwissProt\n",
        "swissprot_fasta = \"uniprot_sprot.fasta\"\n",
        "\n",
        "# Load SwissProt sequences and randomly sample 1000\n",
        "sequences = list(SeqIO.parse(swissprot_fasta, \"fasta\"))\n",
        "\n",
        "# Sample 1000 sequences\n",
        "sampled_sequences = random.sample(sequences, 100)\n",
        "\n",
        "# Extract UniProt IDs (they are in the FASTA header)\n",
        "uniprot_ids = [seq.id.split('|')[1] for seq in sampled_sequences]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS7Wsz1bLpqJ"
      },
      "source": [
        "### Download AlphaFold structures using Bio.PDB.alphafold_db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cFly7GLr34",
        "outputId": "4f8199b9-de5f-4c3a-f6ae-be4e3c9f3fa0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded AlphaFold structure for Q4L6X8\n",
            "Downloaded AlphaFold structure for Q8E3Y3\n",
            "Downloaded AlphaFold structure for Q66I71\n",
            "Downloaded AlphaFold structure for Q6C6N0\n",
            "Downloaded AlphaFold structure for Q6NDE9\n",
            "Downloaded AlphaFold structure for Q9ASR3\n",
            "Downloaded AlphaFold structure for Q2Y7P0\n",
            "Downloaded AlphaFold structure for Q3J446\n",
            "Downloaded AlphaFold structure for Q3Z8Z7\n",
            "Downloaded AlphaFold structure for P36438\n",
            "Downloaded AlphaFold structure for B0U0T8\n",
            "Downloaded AlphaFold structure for A4R962\n",
            "Downloaded AlphaFold structure for Q8N1N0\n",
            "Failed to download structure for Q06658: 404\n",
            "Downloaded AlphaFold structure for P85115\n",
            "Downloaded AlphaFold structure for B3ELD9\n",
            "Downloaded AlphaFold structure for A0RMU4\n",
            "Downloaded AlphaFold structure for Q9FN93\n",
            "Downloaded AlphaFold structure for P57722\n",
            "Downloaded AlphaFold structure for B2S046\n",
            "Downloaded AlphaFold structure for Q65CR5\n",
            "Downloaded AlphaFold structure for P05342\n",
            "Downloaded AlphaFold structure for Q7NNK8\n",
            "Downloaded AlphaFold structure for Q8X723\n",
            "Downloaded AlphaFold structure for Q6FJL9\n",
            "Downloaded AlphaFold structure for Q2YUY9\n",
            "Downloaded AlphaFold structure for B7UMD5\n",
            "Downloaded AlphaFold structure for Q2HJA8\n",
            "Downloaded AlphaFold structure for Q8DLG7\n",
            "Downloaded AlphaFold structure for Q14JL2\n",
            "Downloaded AlphaFold structure for Q9XVS8\n",
            "Downloaded AlphaFold structure for A5GVW5\n",
            "Failed to download structure for P16489: 404\n",
            "Downloaded AlphaFold structure for A0PZC6\n",
            "Downloaded AlphaFold structure for A5UE41\n",
            "Downloaded AlphaFold structure for Q58460\n",
            "Downloaded AlphaFold structure for C1CRK3\n",
            "Downloaded AlphaFold structure for A2S5P8\n",
            "Downloaded AlphaFold structure for Q8U8J4\n",
            "Downloaded AlphaFold structure for Q55338\n",
            "Failed to download structure for P60166: 404\n",
            "Downloaded AlphaFold structure for Q0SZN3\n",
            "Downloaded AlphaFold structure for Q5HFF2\n",
            "Downloaded AlphaFold structure for Q55054\n",
            "Downloaded AlphaFold structure for P30531\n",
            "Downloaded AlphaFold structure for Q8E7Q7\n",
            "Downloaded AlphaFold structure for A4TJI3\n",
            "Downloaded AlphaFold structure for A8YUE3\n",
            "Downloaded AlphaFold structure for Q0HDC7\n",
            "Downloaded AlphaFold structure for Q5XCX9\n",
            "Downloaded AlphaFold structure for P45094\n",
            "Downloaded AlphaFold structure for A0A161X1M2\n",
            "Downloaded AlphaFold structure for B4I609\n",
            "Downloaded AlphaFold structure for Q9TTI8\n",
            "Downloaded AlphaFold structure for Q66CF1\n",
            "Downloaded AlphaFold structure for A1R7V6\n",
            "Failed to download structure for Q19QT7: 404\n",
            "Downloaded AlphaFold structure for Q04B92\n",
            "Downloaded AlphaFold structure for Q37430\n",
            "Downloaded AlphaFold structure for Q7UKG9\n",
            "Downloaded AlphaFold structure for Q7XUR2\n",
            "Downloaded AlphaFold structure for Q23757\n",
            "Downloaded AlphaFold structure for C4L7W3\n",
            "Downloaded AlphaFold structure for O45595\n",
            "Downloaded AlphaFold structure for Q2JFG5\n",
            "Downloaded AlphaFold structure for Q6MII5\n",
            "Downloaded AlphaFold structure for A7X2A2\n",
            "Downloaded AlphaFold structure for P64489\n",
            "Downloaded AlphaFold structure for A2RVM0\n",
            "Downloaded AlphaFold structure for A4TEC0\n",
            "Downloaded AlphaFold structure for P30181\n",
            "Downloaded AlphaFold structure for O35218\n",
            "Downloaded AlphaFold structure for Q31DD8\n",
            "Downloaded AlphaFold structure for Q7MPI9\n",
            "Downloaded AlphaFold structure for P83366\n",
            "Downloaded AlphaFold structure for Q9LMB0\n",
            "Downloaded AlphaFold structure for O82370\n",
            "Downloaded AlphaFold structure for C5FY68\n",
            "Downloaded AlphaFold structure for A1WWB6\n",
            "Downloaded AlphaFold structure for Q62314\n",
            "Downloaded AlphaFold structure for Q5M674\n",
            "Downloaded AlphaFold structure for B8GV40\n",
            "Downloaded AlphaFold structure for A4FUH1\n",
            "Downloaded AlphaFold structure for A0PXX5\n",
            "Downloaded AlphaFold structure for A1R8U1\n",
            "Failed to download structure for Q9XNX1: 404\n",
            "Downloaded AlphaFold structure for Q2RMR9\n",
            "Downloaded AlphaFold structure for Q5ZKK0\n",
            "Downloaded AlphaFold structure for O74405\n",
            "Downloaded AlphaFold structure for P17340\n",
            "Downloaded AlphaFold structure for E9PSL7\n",
            "Downloaded AlphaFold structure for Q9PC57\n",
            "Failed to download structure for P06919: 404\n",
            "Downloaded AlphaFold structure for Q3C1D4\n",
            "Downloaded AlphaFold structure for P9WI47\n",
            "Downloaded AlphaFold structure for Q576P8\n",
            "Downloaded AlphaFold structure for Q32AW3\n",
            "Downloaded AlphaFold structure for Q9PLN5\n",
            "Downloaded AlphaFold structure for B8HX01\n",
            "Downloaded AlphaFold structure for Q9P7Q8\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "# Directory to store AlphaFold structures\n",
        "output_dir = \"alphafold_structures\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Function to download AlphaFold structure from API\n",
        "def download_alphafold_structure(uniprot_id, output_dir):\n",
        "    url = f\"https://alphafold.com/api/prediction/{uniprot_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        response = requests.get(response.json()[0]['pdbUrl'])\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to download structure for {uniprot_id}: {response.status_code}\")\n",
        "            return None\n",
        "        output_file = os.path.join(output_dir, f\"{uniprot_id}.pdb\")\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Downloaded AlphaFold structure for {uniprot_id}\")\n",
        "        return output_file\n",
        "    else:\n",
        "        print(f\"Failed to download structure for {uniprot_id}: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Download AlphaFold structures for sampled UniProt IDs\n",
        "pdb_files = []\n",
        "for uniprot_id in uniprot_ids:\n",
        "    pdb_file = download_alphafold_structure(uniprot_id, output_dir)\n",
        "    if pdb_file:\n",
        "        pdb_files.append(pdb_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj743kgTg-Sj",
        "outputId": "09e635dc-c6b2-4ce7-8377-f07273f5048b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt-get\n"
          ]
        }
      ],
      "source": [
        "!apt-get install dssp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_1clq9-NWiP"
      },
      "source": [
        "### Compute Secondary Structures using DSSP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DCi3WlhNO8O",
        "outputId": "6b386872-e8e5-4a32-fc7b-9db5e801f175"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/Bio/PDB/DSSP.py:199: UserWarning: Trying to parse 'DAUC'\n",
            "Error parsing PDB at line 38\n",
            "Not a valid integer in PDB record\n",
            "\n",
            "  warnings.warn(err)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error in computing secondary structure for  alphafold_structures/A0A161X1M2.pdb\n",
            "Secondary structure computation completed and saved.\n"
          ]
        }
      ],
      "source": [
        "from Bio.PDB import PDBParser\n",
        "from Bio.PDB import DSSP\n",
        "\n",
        "parser = PDBParser(QUIET=True)\n",
        "\n",
        "# Dictionary to store UniProt ID and secondary structure assignment\n",
        "secondary_structures = {}\n",
        "\n",
        "for pdb_file in pdb_files:\n",
        "    try:\n",
        "        # Parse the AlphaFold structure\n",
        "        structure = parser.get_structure(\"model\", pdb_file)\n",
        "        model = structure[0]\n",
        "\n",
        "        # Apply DSSP to the AlphaFold model\n",
        "        dssp = DSSP(model, pdb_file)\n",
        "\n",
        "        # Extract secondary structure assignment\n",
        "        sec_struct = [item[2] for item in dssp]\n",
        "        uniprot_id = os.path.basename(pdb_file).split('.')[0]\n",
        "        secondary_structures[uniprot_id] = sec_struct\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error in computing secondary structure for ', pdb_file)\n",
        "# Save secondary structures to file\n",
        "import json\n",
        "with open(\"secondary_structures.json\", \"w\") as f:\n",
        "    json.dump(secondary_structures, f)\n",
        "\n",
        "print(\"Secondary structure computation completed and saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNnT5wB4LGS2"
      },
      "source": [
        "### Encoding Sequences and Structures\n",
        "\n",
        "Similar to Chapter 3, we will **one-hot encode** the amino acid sequences. The secondary structure labels will also be encoded, but instead of one-hot encoding, we can map each label to a unique integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K3NXXCfDJdn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Define amino acids and secondary structure mappings\n",
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "aa_to_int = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "structure_to_int = {'H': 0, 'E': 1, 'T': 2, 'B': 3, 'S': 4, 'G': 5, 'P': 6, 'I': 7, '-': 8}\n",
        "\n",
        "# Function to read sequences from a FASTA file\n",
        "def read_fasta_to_dict(fasta_file):\n",
        "    seq_dict = {}\n",
        "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "        seq_dict[record.id.split('|')[1]] = str(record.seq)\n",
        "    return seq_dict\n",
        "\n",
        "# Function to encode sequences\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    one_hot = np.zeros((len(sequence), len(amino_acids)))\n",
        "    for i, aa in enumerate(sequence):\n",
        "        if aa in aa_to_int:\n",
        "            one_hot[i, aa_to_int[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Function to encode secondary structure\n",
        "def encode_structure(structure):\n",
        "    one_hot = np.zeros((len(structure), len(structure_to_int)))\n",
        "    for i, ss in enumerate(structure):\n",
        "        if ss in structure_to_int:\n",
        "            one_hot[i, structure_to_int[ss]] = 1\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "# Function to pad or truncate sequences\n",
        "def pad_or_truncate_sequence(sequence, target_length):\n",
        "    if len(sequence) > target_length:\n",
        "        return sequence[:target_length]\n",
        "    elif len(sequence) < target_length:\n",
        "        padding_length = target_length - len(sequence)\n",
        "        return np.pad(sequence, ((0, padding_length), (0, 0)), 'constant')\n",
        "    else:\n",
        "        return sequence\n",
        "\n",
        "\n",
        "# Example usage\n",
        "fasta_file = \"uniprot_sprot.fasta\"  # Path to your FASTA file\n",
        "sequence_dict = read_fasta_to_dict(fasta_file)\n",
        "\n",
        "# Initialize lists to store the processed data\n",
        "X_windows = []\n",
        "encoded_structure = []\n",
        "\n",
        "# Define a fixed window size\n",
        "window_size = 100  # You can choose the appropriate size based on your data\n",
        "\n",
        "for uniprot_id, ss in secondary_structures.items():\n",
        "\n",
        "    if uniprot_id in sequence_dict:\n",
        "        sequence = sequence_dict[uniprot_id]\n",
        "\n",
        "        # One-hot encode the amino acid sequence\n",
        "        encoded_sequence = one_hot_encode_sequence(sequence)\n",
        "\n",
        "        # Pad or truncate to ensure fixed length\n",
        "        padded_sequence = pad_or_truncate_sequence(encoded_sequence, window_size)\n",
        "\n",
        "        X_windows.append(padded_sequence)\n",
        "        encoded_structure.append(pad_or_truncate_sequence(encode_structure(ss), window_size))\n",
        "\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_windows = np.array(X_windows)  # Shape: (num_samples, window_size, num_features)\n",
        "encoded_structure = np.array(encoded_structure)  # Shape: (num_samples, window_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xGnMM-hRSZD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_windows, encoded_structure, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VjDQRNDpjY"
      },
      "source": [
        "## Deep Learning for Secondary Structure Prediction\n",
        "\n",
        "Deep learning models, such as **Recurrent Neural Networks (RNNs)** or **Convolutional Neural Networks (CNNs)**, can capture sequential patterns more effectively for this type of task.\n",
        "\n",
        "### LSTM (Long Short-Term Memory) Model\n",
        "\n",
        "LSTMs are a type of RNN that can capture long-range dependencies, making them a powerful choice for sequence-based tasks like secondary structure prediction.\n",
        "\n",
        "In this notebook, we will implement a deep learning model using PyTorch Lightning, a framework that simplifies the training and evaluation process for PyTorch models. PyTorch Lightning helps manage the boilerplate code, making it easier to scale models and manage experiments.\n",
        "\n",
        "### PyTorch Lightning Setup\n",
        "The LSTM model will be wrapped inside a PyTorch Lightning module, which handles the forward pass, training, and validation steps for us. This module will allow us to train, validate, and predict efficiently while maintaining flexibility for future modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3FsKSIMGIDY",
        "outputId": "9e400576-0f82-4265-a54c-54c879058c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-lightning in ./ccourse/lib/python3.9/site-packages (2.4.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in ./ccourse/lib/python3.9/site-packages (from pytorch-lightning) (2.4.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in ./ccourse/lib/python3.9/site-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in ./ccourse/lib/python3.9/site-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in ./ccourse/lib/python3.9/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.9.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in ./ccourse/lib/python3.9/site-packages (from pytorch-lightning) (1.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./ccourse/lib/python3.9/site-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in ./ccourse/lib/python3.9/site-packages (from pytorch-lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in ./ccourse/lib/python3.9/site-packages (from pytorch-lightning) (0.11.7)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./ccourse/lib/python3.9/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\n",
            "Requirement already satisfied: setuptools in ./ccourse/lib/python3.9/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: filelock in ./ccourse/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in ./ccourse/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in ./ccourse/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in ./ccourse/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>1.20.0 in ./ccourse/lib/python3.9/site-packages (from torchmetrics>=0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./ccourse/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./ccourse/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./ccourse/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./ccourse/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./ccourse/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./ccourse/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./ccourse/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./ccourse/lib/python3.9/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./ccourse/lib/python3.9/site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in ./ccourse/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYqDyCWcIv7r",
        "outputId": "d0bb5d53-645a-4858-8d6c-d4194d26b6ad",
        "colab": {
          "referenced_widgets": [
            "f9bdd77a84ba4b5bb1f2f62d2bf03905",
            "5138575580be4b638ac48e5451347887",
            "84616e974bf449a5991a5bf797cd321d",
            "99cf7534ddbe47e18ec008b2c4e1b40e",
            "b0b9d516699d4a2b96f10273be934c33",
            "f2fe74ceb4764e0c93fa41dee0ad70a9",
            "5f45319f566b42809f8151c1d739b599",
            "ccd5c90358fe4deb8ca830c0cf0fb840",
            "4c405d400cd347d4b6f81c7bbf0f93ab",
            "c56c5fc6ac28442db17a0aad355133a4",
            "845c29c1bf24496fb304e6ab4fe2cecb",
            "aae4de5f452a46258391a1bc5608d480",
            "3c4c1686057f4227b2a1be2a84a1e86f"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name     | Type               | Params | Mode \n",
            "--------------------------------------------------------\n",
            "0 | lstm     | LSTM               | 22.0 K | train\n",
            "1 | fc       | Linear             | 585    | train\n",
            "2 | loss_fn  | CrossEntropyLoss   | 0      | train\n",
            "3 | accuracy | MulticlassAccuracy | 0      | train\n",
            "--------------------------------------------------------\n",
            "22.6 K    Trainable params\n",
            "0         Non-trainable params\n",
            "22.6 K    Total params\n",
            "0.090     Total estimated model params size (MB)\n",
            "4         Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9bdd77a84ba4b5bb1f2f62d2bf03905",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5138575580be4b638ac48e5451347887",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84616e974bf449a5991a5bf797cd321d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99cf7534ddbe47e18ec008b2c4e1b40e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0b9d516699d4a2b96f10273be934c33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2fe74ceb4764e0c93fa41dee0ad70a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f45319f566b42809f8151c1d739b599",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd5c90358fe4deb8ca830c0cf0fb840",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c405d400cd347d4b6f81c7bbf0f93ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c56c5fc6ac28442db17a0aad355133a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "845c29c1bf24496fb304e6ab4fe2cecb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aae4de5f452a46258391a1bc5608d480",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c4c1686057f4227b2a1be2a84a1e86f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Validate metric           DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "         val_acc             0.277368426322937\n",
            "        val_loss            1.8194397687911987\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'val_loss': 1.8194397687911987, 'val_acc': 0.277368426322937}]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Define the PyTorch Lightning module\n",
        "class LSTMModel(pl.LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=1e-3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=output_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out)  # Apply fully connected layer to each time step\n",
        "        return out # Shape: (batch_size, window_size, output_size)\n",
        "\n",
        "    # Training step (one batch)\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        # Reshape outputs and labels for loss calculation (flatten time steps and batch size)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # (batch_size * window_size, output_size)\n",
        "        labels = labels.view(-1, labels.shape[-1])  # (batch_size * window_size)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = self.accuracy(torch.argmax(outputs, dim=1), torch.argmax(labels, dim=1))\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # Validation step (one batch)\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1, labels.shape[-1])\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = self.accuracy(torch.argmax(outputs, dim=1), torch.argmax(labels, dim=1))\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "    # Predict method to return predicted labels\n",
        "    def predict(self, x):\n",
        "        self.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            outputs = self(x)  # Forward pass\n",
        "            predicted_labels = torch.argmax(outputs, dim=2)  # Get predicted class (max value along class dimension)\n",
        "        return predicted_labels\n",
        "\n",
        "    # Optimizer setup\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "input_size = X_windows.shape[2]  # Number of features (e.g., amino acids)\n",
        "hidden_size = 64  # Number of LSTM units\n",
        "output_size = 9   # Number of classes\n",
        "\n",
        "# Create an instance of the model\n",
        "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "# Initialize the PyTorch Lightning Trainer\n",
        "trainer = pl.Trainer(max_epochs=10, accelerator=\"auto\", devices=\"auto\")  # Automatically uses GPU if available\n",
        "\n",
        "# Train the model using the Trainer (like model.fit)\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "# Optionally, evaluate the model on a test set (similar to model.evaluate)\n",
        "trainer.validate(model, dataloaders=val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmRuhohTDtYT"
      },
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "We can evaluate the performance of our models using metrics such as **accuracy**, **precision**, and **recall**. In secondary structure prediction, it's important to measure the performance for each class individually as well as the overall performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "549Xfc7DDvjx",
        "outputId": "d60805ea-1255-41c1-f1f3-000a1376f1a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM Classification Report:\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "      Helix (H)       0.36      0.86      0.51       528\n",
            "      Sheet (E)       0.40      0.34      0.37       381\n",
            "       Coil (C)       0.30      0.02      0.03       185\n",
            "       Turn (T)       0.00      0.00      0.00        13\n",
            "       Bend (B)       0.00      0.00      0.00       143\n",
            "     Bridge (G)       0.00      0.00      0.00        37\n",
            "Polyproline (P)       0.00      0.00      0.00        94\n",
            "    I-helix (I)       0.00      0.00      0.00        10\n",
            "      Other (-)       0.48      0.31      0.38       509\n",
            "\n",
            "       accuracy                           0.39      1900\n",
            "      macro avg       0.17      0.17      0.14      1900\n",
            "   weighted avg       0.34      0.39      0.32      1900\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/Users/ozdeniz/Desktop/crashcourse/protein-ml-crash-course/ccourse/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Get predictions for the validation data\n",
        "predictions = model.predict(X_val_tensor)\n",
        "\n",
        "# LSTM Evaluation (for demonstration, evaluating on training data)\n",
        "lstm_pred = model.predict(X_val_tensor)\n",
        "\n",
        "# Flatten both predictions and true labels to compare them element-wise\n",
        "lstm_pred_flat = lstm_pred.view(-1).cpu().numpy()  # Flatten the predicted labels and move to CPU\n",
        "y_val_flat = torch.argmax(y_val_tensor, dim = 2) # Flatten the true labels and move to CPU\n",
        "y_val_flat = y_val_flat.view(-1).cpu().numpy()\n",
        "# Generate the classification report\n",
        "target_names = ['Helix (H)', 'Sheet (E)', 'Coil (C)', 'Turn (T)', 'Bend (B)', 'Bridge (G)', 'Polyproline (P)', 'I-helix (I)', 'Other (-)']\n",
        "lstm_report = classification_report(y_val_flat, lstm_pred_flat, target_names=target_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"LSTM Classification Report:\\n\", lstm_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-NDlQGDybB"
      },
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "In this chapter, we tackled the problem of secondary structure prediction and demonstrated how to create a dataset from UniProt SwissProt. We then explored advanced deep learning architectures, specifically LSTMs (Long Short-Term Memory networks), to predict protein secondary structures based on amino acid sequences. Additionally, we introduced the use of PyTorch Lightning to streamline the training and evaluation process, making our deep learning workflow more efficient and scalable."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "ccourse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}