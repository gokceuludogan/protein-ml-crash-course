{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLPLjwqGz3s2zGtzQiFJQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokceuludogan/protein-ml-crash-course/blob/wip/Chapter_4_Secondary_Structure_Prediction_with_LSTM_model_%26_Pytorch_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this chapter, we will predict **protein secondary structure**, which refers to the local folding of the protein's backbone into common motifs such as **alpha helices**, **beta sheets**, and **random coils**. This is a classic problem in bioinformatics that can be addressed using machine learning (ML) or deep learning (DL) techniques.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Understand the protein secondary structure prediction task.\n",
        "- Encode protein sequences and their corresponding secondary structure labels.\n",
        "- Use both traditional ML and DL techniques to build predictive models.\n",
        "- Evaluate the performance of these models.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Protein Secondary Structure\n",
        "\n",
        "### Definitions:\n",
        "\n",
        "- **Primary structure**: The linear sequence of amino acids in the protein.\n",
        "- **Secondary structure**: Local folding patterns of the protein chain, mainly forming alpha helices (H), beta sheets (E), or random coils (C).\n",
        "- **Tertiary structure**: The overall 3D structure of the protein.\n",
        "\n",
        "### Common Classes in Secondary Structure Prediction:\n",
        "\n",
        "- **H**: Alpha-helix\n",
        "- **E**: Beta-sheet\n",
        "- **C**: Coil (random structure)\n",
        "\n",
        "### Dataset Example:\n",
        "\n",
        "We will work with a dataset where each protein sequence is accompanied by its secondary structure labels.\n",
        "\n",
        "| Sequence | Secondary Structure |\n",
        "| --- | --- |\n",
        "| MAGWELV | HCCCCCC |\n",
        "| GGQVNLL | CCECCEE |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Data Preprocessing\n",
        "\n",
        "For this task, we will use the UniProt SwissProt database, which contains manually curated protein sequences. We will download the sequences in FASTA format and compute their secondary structures using DSSP (Define Secondary Structure of Proteins), a tool that assigns secondary structure elements (alpha-helix, beta-sheet, coil) based on 3D coordinates of the protein's structure.\n",
        "\n",
        "### Download UniProt SwissProt Database\n",
        "\n",
        "We will download the SwissProt database from the UniProt FTP server. This database contains high-quality, manually reviewed protein sequences.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Dw32R0bDUv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
        "!gunzip uniprot_sprot.fasta.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6IA8kOqK8Kr",
        "outputId": "e758f719-92fa-4f31-a158-30ad243006e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-30 11:20:46--  https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
            "Resolving ftp.uniprot.org (ftp.uniprot.org)... 128.175.240.195\n",
            "Connecting to ftp.uniprot.org (ftp.uniprot.org)|128.175.240.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92500409 (88M) [application/x-gzip]\n",
            "Saving to: ‘uniprot_sprot.fasta.gz’\n",
            "\n",
            "uniprot_sprot.fasta 100%[===================>]  88.21M  96.4MB/s    in 0.9s    \n",
            "\n",
            "2024-09-30 11:20:47 (96.4 MB/s) - ‘uniprot_sprot.fasta.gz’ saved [92500409/92500409]\n",
            "\n",
            "gzip: uniprot_sprot.fasta already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbQqgZ0ZLVVc",
        "outputId": "e71e699e-3be8-4ae7-9234-c4d282a25a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.84)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Set the file path for SwissProt\n",
        "swissprot_fasta = \"uniprot_sprot.fasta\"\n",
        "\n",
        "# Load SwissProt sequences and randomly sample 1000\n",
        "sequences = list(SeqIO.parse(swissprot_fasta, \"fasta\"))\n",
        "\n",
        "# Sample 1000 sequences\n",
        "sampled_sequences = random.sample(sequences, 100)\n",
        "\n",
        "# Extract UniProt IDs (they are in the FASTA header)\n",
        "uniprot_ids = [seq.id.split('|')[1] for seq in sampled_sequences]\n"
      ],
      "metadata": {
        "id": "R0IKZvsdLQKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download AlphaFold structures using Bio.PDB.alphafold_db\n"
      ],
      "metadata": {
        "id": "bS7Wsz1bLpqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "# Directory to store AlphaFold structures\n",
        "output_dir = \"alphafold_structures\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Function to download AlphaFold structure from API\n",
        "def download_alphafold_structure(uniprot_id, output_dir):\n",
        "    url = f\"https://alphafold.com/api/prediction/{uniprot_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        response = requests.get(response.json()[0]['pdbUrl'])\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to download structure for {uniprot_id}: {response.status_code}\")\n",
        "            return None\n",
        "        output_file = os.path.join(output_dir, f\"{uniprot_id}.pdb\")\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Downloaded AlphaFold structure for {uniprot_id}\")\n",
        "        return output_file\n",
        "    else:\n",
        "        print(f\"Failed to download structure for {uniprot_id}: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Download AlphaFold structures for sampled UniProt IDs\n",
        "pdb_files = []\n",
        "for uniprot_id in uniprot_ids:\n",
        "    pdb_file = download_alphafold_structure(uniprot_id, output_dir)\n",
        "    if pdb_file:\n",
        "        pdb_files.append(pdb_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cFly7GLr34",
        "outputId": "9cca9910-72a0-44b3-b649-ff8a0207b88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded AlphaFold structure for Q10V92\n",
            "Downloaded AlphaFold structure for Q8X583\n",
            "Downloaded AlphaFold structure for P0CP79\n",
            "Downloaded AlphaFold structure for Q8VZ20\n",
            "Downloaded AlphaFold structure for Q3A002\n",
            "Downloaded AlphaFold structure for B2JEQ7\n",
            "Downloaded AlphaFold structure for B1IGF7\n",
            "Downloaded AlphaFold structure for A8MB19\n",
            "Downloaded AlphaFold structure for Q54GD8\n",
            "Downloaded AlphaFold structure for P02941\n",
            "Downloaded AlphaFold structure for A8ZTV4\n",
            "Downloaded AlphaFold structure for P35136\n",
            "Downloaded AlphaFold structure for O83405\n",
            "Downloaded AlphaFold structure for O28052\n",
            "Downloaded AlphaFold structure for Q20057\n",
            "Downloaded AlphaFold structure for O79221\n",
            "Downloaded AlphaFold structure for B4RPF1\n",
            "Downloaded AlphaFold structure for Q6DDM4\n",
            "Downloaded AlphaFold structure for Q1X704\n",
            "Downloaded AlphaFold structure for Q1QQS5\n",
            "Downloaded AlphaFold structure for Q54EQ1\n",
            "Downloaded AlphaFold structure for A1U260\n",
            "Downloaded AlphaFold structure for Q86UD3\n",
            "Downloaded AlphaFold structure for Q1CDX2\n",
            "Downloaded AlphaFold structure for Q97CQ1\n",
            "Downloaded AlphaFold structure for B5E3L1\n",
            "Downloaded AlphaFold structure for B3WED2\n",
            "Downloaded AlphaFold structure for Q9PE82\n",
            "Downloaded AlphaFold structure for A8Z434\n",
            "Downloaded AlphaFold structure for P28406\n",
            "Downloaded AlphaFold structure for Q973J1\n",
            "Downloaded AlphaFold structure for P0CO00\n",
            "Downloaded AlphaFold structure for Q73F75\n",
            "Downloaded AlphaFold structure for A4QKS1\n",
            "Downloaded AlphaFold structure for Q8FAW3\n",
            "Downloaded AlphaFold structure for Q8IY17\n",
            "Downloaded AlphaFold structure for P64601\n",
            "Downloaded AlphaFold structure for Q62GB0\n",
            "Downloaded AlphaFold structure for O49420\n",
            "Downloaded AlphaFold structure for Q24K00\n",
            "Failed to download structure for Q02838: 404\n",
            "Downloaded AlphaFold structure for A1BF35\n",
            "Downloaded AlphaFold structure for A1QZD3\n",
            "Downloaded AlphaFold structure for Q9K843\n",
            "Downloaded AlphaFold structure for A3D300\n",
            "Downloaded AlphaFold structure for P39095\n",
            "Downloaded AlphaFold structure for B7JUU6\n",
            "Downloaded AlphaFold structure for A5IBR1\n",
            "Downloaded AlphaFold structure for Q9LM13\n",
            "Downloaded AlphaFold structure for B4TUG2\n",
            "Downloaded AlphaFold structure for A0A084API2\n",
            "Downloaded AlphaFold structure for Q5HPH0\n",
            "Downloaded AlphaFold structure for B1L1E6\n",
            "Downloaded AlphaFold structure for Q8P0C7\n",
            "Downloaded AlphaFold structure for Q6SA96\n",
            "Downloaded AlphaFold structure for P9WQC6\n",
            "Downloaded AlphaFold structure for Q9PK26\n",
            "Downloaded AlphaFold structure for Q1MPL7\n",
            "Downloaded AlphaFold structure for Q9KKY1\n",
            "Downloaded AlphaFold structure for Q9SHZ0\n",
            "Downloaded AlphaFold structure for Q3KEX7\n",
            "Downloaded AlphaFold structure for Q5VTA0\n",
            "Downloaded AlphaFold structure for B7M7M9\n",
            "Downloaded AlphaFold structure for O26118\n",
            "Downloaded AlphaFold structure for P93253\n",
            "Downloaded AlphaFold structure for Q7ICQ6\n",
            "Downloaded AlphaFold structure for B7MEE4\n",
            "Downloaded AlphaFold structure for C8Z891\n",
            "Downloaded AlphaFold structure for A4XI04\n",
            "Downloaded AlphaFold structure for A0T0X9\n",
            "Downloaded AlphaFold structure for C4XPJ3\n",
            "Downloaded AlphaFold structure for A8W3F4\n",
            "Downloaded AlphaFold structure for A5GIS2\n",
            "Failed to download structure for P20512: 404\n",
            "Downloaded AlphaFold structure for Q83LF3\n",
            "Downloaded AlphaFold structure for B0G163\n",
            "Downloaded AlphaFold structure for A5VTB6\n",
            "Downloaded AlphaFold structure for Q9KLX9\n",
            "Downloaded AlphaFold structure for Q9HQS1\n",
            "Downloaded AlphaFold structure for B4RE11\n",
            "Downloaded AlphaFold structure for P61090\n",
            "Downloaded AlphaFold structure for B9LIK3\n",
            "Downloaded AlphaFold structure for Q1JJD0\n",
            "Downloaded AlphaFold structure for Q92EQ6\n",
            "Downloaded AlphaFold structure for C0MGB3\n",
            "Downloaded AlphaFold structure for B2ZBB8\n",
            "Downloaded AlphaFold structure for A6RPU0\n",
            "Downloaded AlphaFold structure for Q0SNF0\n",
            "Downloaded AlphaFold structure for Q5ZRU0\n",
            "Downloaded AlphaFold structure for Q5JFW3\n",
            "Downloaded AlphaFold structure for Q2R2W1\n",
            "Downloaded AlphaFold structure for B2RM93\n",
            "Downloaded AlphaFold structure for Q2PG81\n",
            "Downloaded AlphaFold structure for Q6FSH0\n",
            "Downloaded AlphaFold structure for Q2JDR0\n",
            "Downloaded AlphaFold structure for Q4KKP5\n",
            "Downloaded AlphaFold structure for Q634E9\n",
            "Downloaded AlphaFold structure for Q60700\n",
            "Downloaded AlphaFold structure for Q0TF11\n",
            "Downloaded AlphaFold structure for Q71RC9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install dssp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj743kgTg-Sj",
        "outputId": "5446f590-81a3-4e43-e06c-037ee7c573a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "dssp is already the newest version (4.0.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Secondary Structures using DSSP."
      ],
      "metadata": {
        "id": "w_1clq9-NWiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.PDB import PDBParser\n",
        "from Bio.PDB import DSSP\n",
        "\n",
        "parser = PDBParser(QUIET=True)\n",
        "\n",
        "# Dictionary to store UniProt ID and secondary structure assignment\n",
        "secondary_structures = {}\n",
        "\n",
        "for pdb_file in pdb_files:\n",
        "    try:\n",
        "        # Parse the AlphaFold structure\n",
        "        structure = parser.get_structure(\"model\", pdb_file)\n",
        "        model = structure[0]\n",
        "\n",
        "        # Apply DSSP to the AlphaFold model\n",
        "        dssp = DSSP(model, pdb_file)\n",
        "\n",
        "        # Extract secondary structure assignment\n",
        "        sec_struct = [item[2] for item in dssp]\n",
        "        uniprot_id = os.path.basename(pdb_file).split('.')[0]\n",
        "        secondary_structures[uniprot_id] = sec_struct\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error in computing secondary structure for ', pdb_file)\n",
        "# Save secondary structures to file\n",
        "import json\n",
        "with open(\"secondary_structures.json\", \"w\") as f:\n",
        "    json.dump(secondary_structures, f)\n",
        "\n",
        "print(\"Secondary structure computation completed and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DCi3WlhNO8O",
        "outputId": "717437d4-f7f1-4d71-d690-7573174aa1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Secondary structure computation completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Sequences and Structures\n",
        "\n",
        "Similar to Chapter 3, we will **one-hot encode** the amino acid sequences. The secondary structure labels (H, E, C) will also be encoded, but instead of one-hot encoding, we can map each label to a unique integer."
      ],
      "metadata": {
        "id": "sNnT5wB4LGS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "4K3NXXCfDJdn",
        "outputId": "0580ac56-3ec3-49c9-f648-cd13a59855c6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "argument of type 'int' is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-83c6e1c20acb>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mX_windows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mencoded_structure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_or_truncate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-71-83c6e1c20acb>\u001b[0m in \u001b[0;36mencode_structure\u001b[0;34m(structure)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructure_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mone_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructure_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: argument of type 'int' is not iterable"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Define amino acids and secondary structure mappings\n",
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "aa_to_int = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "structure_to_int = {'H': 0, 'E': 1, 'T': 2, 'B': 3, 'S': 4, 'G': 5, 'P': 6, 'I': 7, '-': 8}\n",
        "\n",
        "# Function to read sequences from a FASTA file\n",
        "def read_fasta_to_dict(fasta_file):\n",
        "    seq_dict = {}\n",
        "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "        seq_dict[record.id.split('|')[1]] = str(record.seq)\n",
        "    return seq_dict\n",
        "\n",
        "# Function to encode sequences\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    one_hot = np.zeros((len(sequence), len(amino_acids)))\n",
        "    for i, aa in enumerate(sequence):\n",
        "        if aa in aa_to_int:\n",
        "            one_hot[i, aa_to_int[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Function to encode secondary structure\n",
        "def encode_structure(structure):\n",
        "    one_hot = np.zeros((len(structure), len(structure_to_int)))\n",
        "    for i, ss in enumerate(structure):\n",
        "        if ss in structure_to_int[ss]:\n",
        "            one_hot[i, structure_to_int[ss]] = 1\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "# Function to pad or truncate sequences\n",
        "def pad_or_truncate_sequence(sequence, target_length):\n",
        "    if len(sequence) > target_length:\n",
        "        return sequence[:target_length]\n",
        "    elif len(sequence) < target_length:\n",
        "        padding_length = target_length - len(sequence)\n",
        "        return np.pad(sequence, ((0, padding_length), (0, 0)), 'constant')\n",
        "    else:\n",
        "        return sequence\n",
        "\n",
        "\n",
        "# Example usage\n",
        "fasta_file = \"uniprot_sprot.fasta\"  # Path to your FASTA file\n",
        "sequence_dict = read_fasta_to_dict(fasta_file)\n",
        "\n",
        "# Initialize lists to store the processed data\n",
        "X_windows = []\n",
        "encoded_structure = []\n",
        "\n",
        "# Define a fixed window size\n",
        "window_size = 100  # You can choose the appropriate size based on your data\n",
        "\n",
        "for uniprot_id, ss in secondary_structures.items():\n",
        "\n",
        "    if uniprot_id in sequence_dict:\n",
        "        sequence = sequence_dict[uniprot_id]\n",
        "\n",
        "        # One-hot encode the amino acid sequence\n",
        "        encoded_sequence = one_hot_encode_sequence(sequence)\n",
        "\n",
        "        # Pad or truncate to ensure fixed length\n",
        "        padded_sequence = pad_or_truncate_sequence(encoded_sequence, window_size)\n",
        "\n",
        "        X_windows.append(padded_sequence)\n",
        "        encoded_structure.append(pad_or_truncate_sequence(encode_structure(ss), window_size))\n",
        "\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_windows = np.array(X_windows)  # Shape: (num_samples, window_size, num_features)\n",
        "encoded_structure = np.array(encoded_structure)  # Shape: (num_samples, window_size)\n",
        "\n",
        "# Flatten encoded_structure if required (you might want to change this)\n",
        "# encoded_structure = encoded_structure.flatten()  # Adjust based on your needs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_windows, encoded_structure, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "3xGnMM-hRSZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning for Secondary Structure Prediction\n",
        "\n",
        "Deep learning models, such as **Recurrent Neural Networks (RNNs)** or **Convolutional Neural Networks (CNNs)**, can capture sequential patterns more effectively for this type of task.\n",
        "\n",
        "### LSTM (Long Short-Term Memory) Model\n",
        "\n",
        "LSTMs are a type of RNN that can capture long-range dependencies, making them a powerful choice for sequence-based tasks like secondary structure prediction."
      ],
      "metadata": {
        "id": "37VjDQRNDpjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "d3FsKSIMGIDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Define the PyTorch Lightning module\n",
        "class LSTMModel(pl.LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=1e-3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=output_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out)  # Apply fully connected layer to each time step\n",
        "        return out # Shape: (batch_size, window_size, output_size)\n",
        "\n",
        "    # Training step (one batch)\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        # Reshape outputs and labels for loss calculation (flatten time steps and batch size)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # (batch_size * window_size, output_size)\n",
        "        labels = labels.view(-1)  # (batch_size * window_size)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = self.accuracy(outputs, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc, prog_bar=True)\n",
        "\n",
        "\n",
        "    # Validation step (one batch)\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = self.accuracy(outputs, labels)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "    # Predict method to return predicted labels\n",
        "    def predict(self, x):\n",
        "        self.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            outputs = self(x)  # Forward pass\n",
        "            predicted_labels = torch.argmax(outputs, dim=1)  # Get predicted class (max value along class dimension)\n",
        "        return predicted_labels\n",
        "\n",
        "    # Optimizer setup\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "input_size = X_windows.shape[2]  # Number of features (e.g., amino acids)\n",
        "hidden_size = 64  # Number of LSTM units\n",
        "output_size = 9   # Number of classes\n",
        "\n",
        "# Create an instance of the model\n",
        "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "# Initialize the PyTorch Lightning Trainer\n",
        "trainer = pl.Trainer(max_epochs=10, accelerator=\"auto\", devices=\"auto\")  # Automatically uses GPU if available\n",
        "\n",
        "# Train the model using the Trainer (like model.fit)\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "# Optionally, evaluate the model on a test set (similar to model.evaluate)\n",
        "trainer.validate(model, dataloaders=val_loader)\n"
      ],
      "metadata": {
        "id": "HYqDyCWcIv7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "We can evaluate the performance of our models using metrics such as **accuracy**, **precision**, and **recall**. In secondary structure prediction, it's important to measure the performance for each class (H, E, C) individually as well as the overall performance."
      ],
      "metadata": {
        "id": "CmRuhohTDtYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Get predictions for the validation data\n",
        "predictions = model.predict(X_val_tensor)\n",
        "\n",
        "# LSTM Evaluation (for demonstration, evaluating on training data)\n",
        "lstm_pred = model.predict(X_val_tensor).cpu().numpy()\n",
        "\n",
        "# Flatten both predictions and true labels to compare them element-wise\n",
        "lstm_pred_flat = lstm_pred.view(-1).cpu().numpy()  # Flatten the predicted labels and move to CPU\n",
        "y_val_flat = y_val_tensor.view(-1).cpu().numpy()   # Flatten the true labels and move to CPU\n",
        "\n",
        "# Generate the classification report\n",
        "target_names = ['Helix (H)', 'Sheet (E)', 'Coil (C)', 'Turn (T)', 'Bend (B)', 'Bridge (G)', 'Polyproline (P)', 'I-helix (I)', 'Other (-)']\n",
        "lstm_report = classification_report(y_val_flat, lstm_pred_flat, target_names=target_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"LSTM Classification Report:\\n\", lstm_report)"
      ],
      "metadata": {
        "id": "549Xfc7DDvjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "In this chapter, we built machine learning and deep learning models to predict protein secondary structure from amino acid sequences. We explored the use of traditional classifiers like Random Forests and advanced deep learning architectures like LSTMs. While traditional methods perform well for small datasets, deep learning models can capture long-range dependencies and improve performance when dealing with larger datasets."
      ],
      "metadata": {
        "id": "3v-NDlQGDybB"
      }
    }
  ]
}