{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokceuludogan/protein-ml-crash-course/blob/wip/Chapter_4_Secondary_Structure_Prediction_with_LSTM_model_%26_Pytorch_Lightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "In this chapter, we will predict **protein secondary structure**, which refers to the local folding of the protein's backbone into common motifs such as **alpha helices**, **beta sheets**, and **random coils**. This is a classic problem in bioinformatics that can be addressed using machine learning (ML) or deep learning (DL) techniques.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Understand the protein secondary structure prediction task.\n",
        "- Encode protein sequences and their corresponding secondary structure labels.\n",
        "- Use both traditional ML and DL techniques to build predictive models.\n",
        "- Evaluate the performance of these models.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Protein Secondary Structure\n",
        "\n",
        "### Definitions:\n",
        "\n",
        "- **Primary structure**: The linear sequence of amino acids in the protein.\n",
        "- **Secondary structure**: Local folding patterns of the protein chain, mainly forming alpha helices (H), beta sheets (E), or random coils (C).\n",
        "- **Tertiary structure**: The overall 3D structure of the protein.\n",
        "\n",
        "### Common Classes in Secondary Structure Prediction:\n",
        "\n",
        "- **H**: Alpha-helix\n",
        "- **E**: Beta-sheet\n",
        "- **C**: Coil (random structure)\n",
        "\n",
        "### Dataset Example:\n",
        "\n",
        "We will work with a dataset where each protein sequence is accompanied by its secondary structure labels.\n",
        "\n",
        "| Sequence | Secondary Structure |\n",
        "| --- | --- |\n",
        "| MAGWELV | HCCCCCC |\n",
        "| GGQVNLL | CCECCEE |\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Data Preprocessing\n",
        "\n",
        "For this task, we will use the UniProt SwissProt database, which contains manually curated protein sequences. We will download the sequences in FASTA format and compute their secondary structures using DSSP (Define Secondary Structure of Proteins), a tool that assigns secondary structure elements (alpha-helix, beta-sheet, coil) based on 3D coordinates of the protein's structure.\n",
        "\n",
        "### Download UniProt SwissProt Database\n",
        "\n",
        "We will download the SwissProt database from the UniProt FTP server. This database contains high-quality, manually reviewed protein sequences.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Dw32R0bDUv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
        "!gunzip uniprot_sprot.fasta.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6IA8kOqK8Kr",
        "outputId": "ed8f5ad6-3436-4e1b-d90a-bbbad8527585"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-03 12:08:34--  https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
            "Resolving ftp.uniprot.org (ftp.uniprot.org)... 128.175.240.195\n",
            "Connecting to ftp.uniprot.org (ftp.uniprot.org)|128.175.240.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 92678508 (88M) [application/x-gzip]\n",
            "Saving to: ‘uniprot_sprot.fasta.gz’\n",
            "\n",
            "uniprot_sprot.fasta 100%[===================>]  88.38M  19.8MB/s    in 6.0s    \n",
            "\n",
            "2024-10-03 12:08:41 (14.7 MB/s) - ‘uniprot_sprot.fasta.gz’ saved [92678508/92678508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbQqgZ0ZLVVc",
        "outputId": "5db92c06-ce4e-4829-9437-d32bab971d48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Set the file path for SwissProt\n",
        "swissprot_fasta = \"uniprot_sprot.fasta\"\n",
        "\n",
        "# Load SwissProt sequences and randomly sample 1000\n",
        "sequences = list(SeqIO.parse(swissprot_fasta, \"fasta\"))\n",
        "\n",
        "# Sample 1000 sequences\n",
        "sampled_sequences = random.sample(sequences, 100)\n",
        "\n",
        "# Extract UniProt IDs (they are in the FASTA header)\n",
        "uniprot_ids = [seq.id.split('|')[1] for seq in sampled_sequences]\n"
      ],
      "metadata": {
        "id": "R0IKZvsdLQKZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download AlphaFold structures using Bio.PDB.alphafold_db\n"
      ],
      "metadata": {
        "id": "bS7Wsz1bLpqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "# Directory to store AlphaFold structures\n",
        "output_dir = \"alphafold_structures\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Function to download AlphaFold structure from API\n",
        "def download_alphafold_structure(uniprot_id, output_dir):\n",
        "    url = f\"https://alphafold.com/api/prediction/{uniprot_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        response = requests.get(response.json()[0]['pdbUrl'])\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to download structure for {uniprot_id}: {response.status_code}\")\n",
        "            return None\n",
        "        output_file = os.path.join(output_dir, f\"{uniprot_id}.pdb\")\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Downloaded AlphaFold structure for {uniprot_id}\")\n",
        "        return output_file\n",
        "    else:\n",
        "        print(f\"Failed to download structure for {uniprot_id}: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Download AlphaFold structures for sampled UniProt IDs\n",
        "pdb_files = []\n",
        "for uniprot_id in uniprot_ids:\n",
        "    pdb_file = download_alphafold_structure(uniprot_id, output_dir)\n",
        "    if pdb_file:\n",
        "        pdb_files.append(pdb_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1cFly7GLr34",
        "outputId": "4f8199b9-de5f-4c3a-f6ae-be4e3c9f3fa0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded AlphaFold structure for Q4ZMP4\n",
            "Downloaded AlphaFold structure for P0A9N5\n",
            "Downloaded AlphaFold structure for C5K105\n",
            "Downloaded AlphaFold structure for Q6YZ99\n",
            "Downloaded AlphaFold structure for Q7TPV4\n",
            "Downloaded AlphaFold structure for P9WF26\n",
            "Failed to download structure for C0HM30: 404\n",
            "Downloaded AlphaFold structure for A8FYJ2\n",
            "Downloaded AlphaFold structure for Q3ATP7\n",
            "Downloaded AlphaFold structure for Q7T0U6\n",
            "Downloaded AlphaFold structure for Q36266\n",
            "Downloaded AlphaFold structure for Q05931\n",
            "Downloaded AlphaFold structure for P84924\n",
            "Downloaded AlphaFold structure for Q5RCH4\n",
            "Downloaded AlphaFold structure for Q7K4H4\n",
            "Downloaded AlphaFold structure for Q8WXX5\n",
            "Downloaded AlphaFold structure for B2SGC2\n",
            "Downloaded AlphaFold structure for P30205\n",
            "Downloaded AlphaFold structure for B3QBV5\n",
            "Downloaded AlphaFold structure for A6ZV04\n",
            "Failed to download structure for Q73PQ2: 404\n",
            "Downloaded AlphaFold structure for A9MFX7\n",
            "Downloaded AlphaFold structure for Q8BU30\n",
            "Downloaded AlphaFold structure for A2BNW7\n",
            "Downloaded AlphaFold structure for A5VQV8\n",
            "Downloaded AlphaFold structure for Q9ZIN3\n",
            "Downloaded AlphaFold structure for Q5R5F6\n",
            "Downloaded AlphaFold structure for A1CLY6\n",
            "Downloaded AlphaFold structure for Q9PCX7\n",
            "Downloaded AlphaFold structure for Q6L394\n",
            "Downloaded AlphaFold structure for Q9FHJ6\n",
            "Downloaded AlphaFold structure for Q6LCI5\n",
            "Downloaded AlphaFold structure for A9KF98\n",
            "Downloaded AlphaFold structure for B7IAF3\n",
            "Downloaded AlphaFold structure for Q96DX4\n",
            "Downloaded AlphaFold structure for A3NCQ8\n",
            "Downloaded AlphaFold structure for Q4A127\n",
            "Downloaded AlphaFold structure for P60151\n",
            "Downloaded AlphaFold structure for A7GRX7\n",
            "Downloaded AlphaFold structure for Q1RG33\n",
            "Downloaded AlphaFold structure for A9VT48\n",
            "Downloaded AlphaFold structure for P0A0P8\n",
            "Downloaded AlphaFold structure for A5ABH0\n",
            "Downloaded AlphaFold structure for Q1XDC6\n",
            "Downloaded AlphaFold structure for B2US06\n",
            "Downloaded AlphaFold structure for A0QIQ3\n",
            "Downloaded AlphaFold structure for P77756\n",
            "Downloaded AlphaFold structure for P94774\n",
            "Downloaded AlphaFold structure for Q9HZA3\n",
            "Downloaded AlphaFold structure for A1A787\n",
            "Downloaded AlphaFold structure for Q663V8\n",
            "Downloaded AlphaFold structure for Q5HGT3\n",
            "Downloaded AlphaFold structure for Q4PSA3\n",
            "Downloaded AlphaFold structure for Q10211\n",
            "Downloaded AlphaFold structure for A4WEY6\n",
            "Downloaded AlphaFold structure for Q9YEQ5\n",
            "Downloaded AlphaFold structure for Q08379\n",
            "Downloaded AlphaFold structure for B3CPU1\n",
            "Downloaded AlphaFold structure for Q94E49\n",
            "Downloaded AlphaFold structure for Q10MN8\n",
            "Downloaded AlphaFold structure for B5YF48\n",
            "Downloaded AlphaFold structure for A8XV36\n",
            "Downloaded AlphaFold structure for A6N9I4\n",
            "Downloaded AlphaFold structure for Q50784\n",
            "Downloaded AlphaFold structure for B9HRL7\n",
            "Downloaded AlphaFold structure for Q55CX9\n",
            "Failed to download structure for P0C9B8: 404\n",
            "Downloaded AlphaFold structure for Q3ZBG0\n",
            "Downloaded AlphaFold structure for Q8W4G6\n",
            "Downloaded AlphaFold structure for A0A7E6FSU6\n",
            "Downloaded AlphaFold structure for Q20EU9\n",
            "Downloaded AlphaFold structure for Q3YVD0\n",
            "Downloaded AlphaFold structure for A7ETJ6\n",
            "Downloaded AlphaFold structure for Q7ZX15\n",
            "Downloaded AlphaFold structure for A2Q9P5\n",
            "Downloaded AlphaFold structure for Q6UWH4\n",
            "Failed to download structure for Q86607: 404\n",
            "Downloaded AlphaFold structure for P28183\n",
            "Downloaded AlphaFold structure for P0CX95\n",
            "Downloaded AlphaFold structure for P41048\n",
            "Downloaded AlphaFold structure for Q5NQN1\n",
            "Downloaded AlphaFold structure for O94651\n",
            "Downloaded AlphaFold structure for Q0K7A0\n",
            "Downloaded AlphaFold structure for Q3HRP5\n",
            "Downloaded AlphaFold structure for B4HIJ8\n",
            "Downloaded AlphaFold structure for Q9VI85\n",
            "Downloaded AlphaFold structure for P0C1R6\n",
            "Downloaded AlphaFold structure for Q2S528\n",
            "Downloaded AlphaFold structure for P41349\n",
            "Failed to download structure for Q7T6Y3: 404\n",
            "Failed to download structure for Q6S6N9: 404\n",
            "Downloaded AlphaFold structure for Q255T2\n",
            "Downloaded AlphaFold structure for Q02205\n",
            "Downloaded AlphaFold structure for C3K4L5\n",
            "Downloaded AlphaFold structure for B4TTH0\n",
            "Downloaded AlphaFold structure for Q5FJX0\n",
            "Downloaded AlphaFold structure for B7HPI8\n",
            "Downloaded AlphaFold structure for Q82HR6\n",
            "Downloaded AlphaFold structure for Q1CMJ9\n",
            "Downloaded AlphaFold structure for Q89AW2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install dssp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj743kgTg-Sj",
        "outputId": "09e635dc-c6b2-4ce7-8377-f07273f5048b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcifpp-data libcifpp2\n",
            "The following NEW packages will be installed:\n",
            "  dssp libcifpp-data libcifpp2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,967 kB of archives.\n",
            "After this operation, 15.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcifpp-data all 2.0.5-1build1 [437 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcifpp2 amd64 2.0.5-1build1 [1,019 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dssp amd64 4.0.4-1 [511 kB]\n",
            "Fetched 1,967 kB in 2s (851 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libcifpp-data.\n",
            "(Reading database ... 123620 files and directories currently installed.)\n",
            "Preparing to unpack .../libcifpp-data_2.0.5-1build1_all.deb ...\n",
            "Unpacking libcifpp-data (2.0.5-1build1) ...\n",
            "Selecting previously unselected package libcifpp2:amd64.\n",
            "Preparing to unpack .../libcifpp2_2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libcifpp2:amd64 (2.0.5-1build1) ...\n",
            "Selecting previously unselected package dssp.\n",
            "Preparing to unpack .../dssp_4.0.4-1_amd64.deb ...\n",
            "Unpacking dssp (4.0.4-1) ...\n",
            "Setting up libcifpp-data (2.0.5-1build1) ...\n",
            "Setting up libcifpp2:amd64 (2.0.5-1build1) ...\n",
            "Setting up dssp (4.0.4-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Secondary Structures using DSSP."
      ],
      "metadata": {
        "id": "w_1clq9-NWiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Bio.PDB import PDBParser\n",
        "from Bio.PDB import DSSP\n",
        "\n",
        "parser = PDBParser(QUIET=True)\n",
        "\n",
        "# Dictionary to store UniProt ID and secondary structure assignment\n",
        "secondary_structures = {}\n",
        "\n",
        "for pdb_file in pdb_files:\n",
        "    try:\n",
        "        # Parse the AlphaFold structure\n",
        "        structure = parser.get_structure(\"model\", pdb_file)\n",
        "        model = structure[0]\n",
        "\n",
        "        # Apply DSSP to the AlphaFold model\n",
        "        dssp = DSSP(model, pdb_file)\n",
        "\n",
        "        # Extract secondary structure assignment\n",
        "        sec_struct = [item[2] for item in dssp]\n",
        "        uniprot_id = os.path.basename(pdb_file).split('.')[0]\n",
        "        secondary_structures[uniprot_id] = sec_struct\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error in computing secondary structure for ', pdb_file)\n",
        "# Save secondary structures to file\n",
        "import json\n",
        "with open(\"secondary_structures.json\", \"w\") as f:\n",
        "    json.dump(secondary_structures, f)\n",
        "\n",
        "print(\"Secondary structure computation completed and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DCi3WlhNO8O",
        "outputId": "6b386872-e8e5-4a32-fc7b-9db5e801f175"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Bio/PDB/DSSP.py:199: UserWarning: Trying to parse 'OCTV'\n",
            "Error parsing PDB at line 38\n",
            "Error trying to load file \"alphafold_structures/A0A7E6FSU6.pdb\"\n",
            "Not a valid integer in PDB record\n",
            "\n",
            "  warnings.warn(err)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in computing secondary structure for  alphafold_structures/A0A7E6FSU6.pdb\n",
            "Secondary structure computation completed and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Sequences and Structures\n",
        "\n",
        "Similar to Chapter 3, we will **one-hot encode** the amino acid sequences. The secondary structure labels (H, E, C) will also be encoded, but instead of one-hot encoding, we can map each label to a unique integer."
      ],
      "metadata": {
        "id": "sNnT5wB4LGS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4K3NXXCfDJdn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Define amino acids and secondary structure mappings\n",
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "aa_to_int = {aa: i for i, aa in enumerate(amino_acids)}\n",
        "structure_to_int = {'H': 0, 'E': 1, 'T': 2, 'B': 3, 'S': 4, 'G': 5, 'P': 6, 'I': 7, '-': 8}\n",
        "\n",
        "# Function to read sequences from a FASTA file\n",
        "def read_fasta_to_dict(fasta_file):\n",
        "    seq_dict = {}\n",
        "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "        seq_dict[record.id.split('|')[1]] = str(record.seq)\n",
        "    return seq_dict\n",
        "\n",
        "# Function to encode sequences\n",
        "def one_hot_encode_sequence(sequence):\n",
        "    one_hot = np.zeros((len(sequence), len(amino_acids)))\n",
        "    for i, aa in enumerate(sequence):\n",
        "        if aa in aa_to_int:\n",
        "            one_hot[i, aa_to_int[aa]] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Function to encode secondary structure\n",
        "def encode_structure(structure):\n",
        "    one_hot = np.zeros((len(structure), len(structure_to_int)))\n",
        "    for i, ss in enumerate(structure):\n",
        "        if ss in structure_to_int:\n",
        "            one_hot[i, structure_to_int[ss]] = 1\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "# Function to pad or truncate sequences\n",
        "def pad_or_truncate_sequence(sequence, target_length):\n",
        "    if len(sequence) > target_length:\n",
        "        return sequence[:target_length]\n",
        "    elif len(sequence) < target_length:\n",
        "        padding_length = target_length - len(sequence)\n",
        "        return np.pad(sequence, ((0, padding_length), (0, 0)), 'constant')\n",
        "    else:\n",
        "        return sequence\n",
        "\n",
        "\n",
        "# Example usage\n",
        "fasta_file = \"uniprot_sprot.fasta\"  # Path to your FASTA file\n",
        "sequence_dict = read_fasta_to_dict(fasta_file)\n",
        "\n",
        "# Initialize lists to store the processed data\n",
        "X_windows = []\n",
        "encoded_structure = []\n",
        "\n",
        "# Define a fixed window size\n",
        "window_size = 100  # You can choose the appropriate size based on your data\n",
        "\n",
        "for uniprot_id, ss in secondary_structures.items():\n",
        "\n",
        "    if uniprot_id in sequence_dict:\n",
        "        sequence = sequence_dict[uniprot_id]\n",
        "\n",
        "        # One-hot encode the amino acid sequence\n",
        "        encoded_sequence = one_hot_encode_sequence(sequence)\n",
        "\n",
        "        # Pad or truncate to ensure fixed length\n",
        "        padded_sequence = pad_or_truncate_sequence(encoded_sequence, window_size)\n",
        "\n",
        "        X_windows.append(padded_sequence)\n",
        "        encoded_structure.append(pad_or_truncate_sequence(encode_structure(ss), window_size))\n",
        "\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_windows = np.array(X_windows)  # Shape: (num_samples, window_size, num_features)\n",
        "encoded_structure = np.array(encoded_structure)  # Shape: (num_samples, window_size)\n",
        "\n",
        "# Flatten encoded_structure if required (you might want to change this)\n",
        "# encoded_structure = encoded_structure.flatten()  # Adjust based on your needs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_windows, encoded_structure, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "3xGnMM-hRSZD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning for Secondary Structure Prediction\n",
        "\n",
        "Deep learning models, such as **Recurrent Neural Networks (RNNs)** or **Convolutional Neural Networks (CNNs)**, can capture sequential patterns more effectively for this type of task.\n",
        "\n",
        "### LSTM (Long Short-Term Memory) Model\n",
        "\n",
        "LSTMs are a type of RNN that can capture long-range dependencies, making them a powerful choice for sequence-based tasks like secondary structure prediction."
      ],
      "metadata": {
        "id": "37VjDQRNDpjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning"
      ],
      "metadata": {
        "id": "d3FsKSIMGIDY",
        "outputId": "9e400576-0f82-4265-a54c-54c879058c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.7 pytorch-lightning-2.4.0 torchmetrics-1.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pytorch_lightning as pl\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Define the PyTorch Lightning module\n",
        "class LSTMModel(pl.LightningModule):\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=1e-3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=output_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out)  # Apply fully connected layer to each time step\n",
        "        return out # Shape: (batch_size, window_size, output_size)\n",
        "\n",
        "    # Training step (one batch)\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        # Reshape outputs and labels for loss calculation (flatten time steps and batch size)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # (batch_size * window_size, output_size)\n",
        "        labels = labels.view(-1, labels.shape[-1])  # (batch_size * window_size)\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = self.accuracy(outputs, labels)\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    # Validation step (one batch)\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        labels = labels.view(-1, labels.shape[-1])\n",
        "        loss = self.loss_fn(outputs, labels)\n",
        "        acc = self.accuracy(outputs, labels)\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "    # Predict method to return predicted labels\n",
        "    def predict(self, x):\n",
        "        self.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            outputs = self(x)  # Forward pass\n",
        "            predicted_labels = torch.argmax(outputs, dim=1)  # Get predicted class (max value along class dimension)\n",
        "        return predicted_labels\n",
        "\n",
        "    # Optimizer setup\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "# Define the model\n",
        "input_size = X_windows.shape[2]  # Number of features (e.g., amino acids)\n",
        "hidden_size = 64  # Number of LSTM units\n",
        "output_size = 9   # Number of classes\n",
        "\n",
        "# Create an instance of the model\n",
        "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "# Initialize the PyTorch Lightning Trainer\n",
        "trainer = pl.Trainer(max_epochs=10, accelerator=\"auto\", devices=\"auto\")  # Automatically uses GPU if available\n",
        "\n",
        "# Train the model using the Trainer (like model.fit)\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "\n",
        "# Optionally, evaluate the model on a test set (similar to model.evaluate)\n",
        "trainer.validate(model, dataloaders=val_loader)\n"
      ],
      "metadata": {
        "id": "HYqDyCWcIv7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "We can evaluate the performance of our models using metrics such as **accuracy**, **precision**, and **recall**. In secondary structure prediction, it's important to measure the performance for each class (H, E, C) individually as well as the overall performance."
      ],
      "metadata": {
        "id": "CmRuhohTDtYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Get predictions for the validation data\n",
        "predictions = model.predict(X_val_tensor)\n",
        "\n",
        "# LSTM Evaluation (for demonstration, evaluating on training data)\n",
        "lstm_pred = model.predict(X_val_tensor).cpu().numpy()\n",
        "\n",
        "# Flatten both predictions and true labels to compare them element-wise\n",
        "lstm_pred_flat = lstm_pred.view(-1).cpu().numpy()  # Flatten the predicted labels and move to CPU\n",
        "y_val_flat = y_val_tensor.view(-1).cpu().numpy()   # Flatten the true labels and move to CPU\n",
        "\n",
        "# Generate the classification report\n",
        "target_names = ['Helix (H)', 'Sheet (E)', 'Coil (C)', 'Turn (T)', 'Bend (B)', 'Bridge (G)', 'Polyproline (P)', 'I-helix (I)', 'Other (-)']\n",
        "lstm_report = classification_report(y_val_flat, lstm_pred_flat, target_names=target_names)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"LSTM Classification Report:\\n\", lstm_report)"
      ],
      "metadata": {
        "id": "549Xfc7DDvjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusion\n",
        "\n",
        "In this chapter, we built machine learning and deep learning models to predict protein secondary structure from amino acid sequences. We explored the use of traditional classifiers like Random Forests and advanced deep learning architectures like LSTMs. While traditional methods perform well for small datasets, deep learning models can capture long-range dependencies and improve performance when dealing with larger datasets."
      ],
      "metadata": {
        "id": "3v-NDlQGDybB"
      }
    }
  ]
}