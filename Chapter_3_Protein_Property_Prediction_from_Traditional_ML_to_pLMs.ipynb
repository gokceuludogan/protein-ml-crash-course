{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLxa/Hz19keayWEqf+BSE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokceuludogan/protein-ml-crash-course/blob/main/Chapter_3_Protein_Property_Prediction_from_Traditional_ML_to_pLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "\n",
        "In this chapter, we will focus on predicting protein subcellular localization, specifically aiming to solve a **binary classification problem**: determining whether a protein is localized in the **cytoplasm** or not. We will begin with a simple machine learning model, advance to a **Convolutional Neural Network (CNN)** built using PyTorch, and finally, we will fine-tune a large pre-trained **transformer model**, **ESM2-650M**. Each approach offers unique insights and challenges, giving us a well-rounded understanding of the problem.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "We'll use the **DeepLoc 2.0** dataset, which provides subcellular localization labels for eukaryotic proteins. The original task is multi-label, with 10 possible localization sites, but we simplify it to a **binary classification**: whether a protein is in the cytoplasm (1) or not (0).\n",
        "\n",
        "---\n",
        "\n",
        "### Data Preprocessing\n",
        "\n",
        "First, we need to preprocess the data, converting the sequences into suitable numeric representations (e.g., one-hot encoding) and extracting features for classification.\n"
      ],
      "metadata": {
        "id": "uYpLAphoNfBD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYmZLoWBNRPI",
        "outputId": "54328fd5-3f6d-4dd8-8671-4bcfbc02a955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-4b7e3e18cd37>:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_val_data['label'] = train_val_data['Cytoplasm']   # Binary label: 1 if Cytoplasm, 0 otherwise\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load datasets\n",
        "train_val_url = \"https://services.healthtech.dtu.dk/services/DeepLoc-2.0/data/Swissprot_Train_Validation_dataset.csv\"\n",
        "test_url = \"https://services.healthtech.dtu.dk/services/DeepLoc-2.0/data/hpa_testset.csv\"\n",
        "\n",
        "train_val_data = pd.read_csv(train_val_url)\n",
        "test_data = pd.read_csv(test_url)\n",
        "\n",
        "\n",
        "# Filter out sequences with 'X' or invalid amino acids\n",
        "train_val_data = train_val_data[~train_val_data['Sequence'].str.contains('X')]\n",
        "# Extract relevant columns: 'Cytoplasm' and 'Sequence'\n",
        "train_val_data['label'] = train_val_data['Cytoplasm']   # Binary label: 1 if Cytoplasm, 0 otherwise\n",
        "\n",
        "# Define the maximum sequence length to truncate\n",
        "MAX_SEQ_LEN = 1000\n",
        "\n",
        "# Map amino acids to integers (A -> 0, C -> 1, ..., Y -> 19)\n",
        "amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "aa_to_int = {aa: idx for idx, aa in enumerate(amino_acids)}\n",
        "\n",
        "# Function to encode and truncate sequences\n",
        "def encode_sequence(seq, max_len):\n",
        "    # Convert sequence to integer encoding\n",
        "    encoded_seq = [aa_to_int[aa] for aa in seq if aa in aa_to_int]\n",
        "\n",
        "    # Truncate or pad the sequence\n",
        "    if len(encoded_seq) > max_len:\n",
        "        return encoded_seq[:max_len]\n",
        "    else:\n",
        "        return encoded_seq + [0] * (max_len - len(encoded_seq))  # Pad with 0s if shorter\n",
        "\n",
        "# Apply encoding and truncating\n",
        "train_val_data['encoded_Sequence'] = train_val_data['Sequence'].apply(lambda x: encode_sequence(x, MAX_SEQ_LEN))\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.stack(train_val_data['encoded_Sequence'].values)\n",
        "y = train_val_data['label'].values\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_seq, X_val_seq, y_train_seq, y_val_seq = train_test_split( train_val_data['Sequence'].values, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 2: Simple Machine Learning Model\n",
        "\n",
        "**Model**: Logistic Regression\n",
        "\n",
        "We start with a logistic regression model, which is a basic yet effective classifier for binary problems. In logistic regression, the model fits a logistic curve to the data and outputs probabilities, which are then used for binary classification. The simplicity of this model makes it interpretable, but it may struggle with highly complex or non-linear patterns, especially when dealing with protein sequences.\n",
        "\n",
        "### Why Logistic Regression?\n",
        "\n",
        "- **Simplicity and Interpretability**: Logistic regression is easy to implement and provides a good baseline. It allows us to interpret the model's output as probabilities, which helps us understand its decision-making process.\n",
        "- **Limitations**: Logistic regression assumes linear relationships, so it might not capture the intricate patterns present in protein sequences."
      ],
      "metadata": {
        "id": "IvykqCjnOCA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train Logistic Regression model\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model on validation data\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJoikISmN_RI",
        "outputId": "64084d56-c30b-487f-f0b4-7a2fdda3c373"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network (CNN)\n",
        "\n",
        "**Model**: 1D Convolutional Neural Network (CNN)\n",
        "\n",
        "The next step is building a **CNN** using PyTorch. CNNs are excellent at learning spatial hierarchies in data. For protein sequences, a 1D CNN can capture local patterns in the sequence, which might correspond to specific motifs or domains related to subcellular localization.\n",
        "\n",
        "### Why a CNN?\n",
        "\n",
        "- **Ability to Capture Local Patterns**: In biological sequences, local patterns or motifs (e.g., specific amino acid combinations) can play a crucial role in protein function and localization. A CNN can efficiently extract these local features.\n",
        "- **Hierarchical Learning**: CNNs use multiple convolutional layers to learn complex patterns, making them well-suited for biological sequence data.\n",
        "- **Drawbacks**: While CNNs can capture local features, they may not be as effective in understanding long-range dependencies in sequences compared to transformers.\n",
        "\n",
        "**Model Architecture**:\n",
        "\n",
        "- **Convolutional Layer**: Extracts local patterns from the protein sequence.\n",
        "- **Max Pooling Layer**: Reduces dimensionality, making the model more efficient.\n",
        "- **Fully Connected Layer**: Combines learned features to make a final binary classification decision."
      ],
      "metadata": {
        "id": "73jIusG1OEzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert X_train and y_train to PyTorch Tensors and Datasets\n",
        "\n",
        "You need to ensure that your input data (X_train) and labels (y_train) are in a format suitable for the CNN. You can then use TensorDataset to combine X_train_tensor and y_train_tensor and create a DataLoader to iterate over the dataset in mini-batches.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HCDyjNdMSE-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.long)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "i5sAyha3RrGh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the CNN Model\n",
        "Now that the data is prepared, we can build a simple CNN for the task."
      ],
      "metadata": {
        "id": "jvjzPMVPv-qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Define constants\n",
        "BATCH_SIZE = 32\n",
        "EMBED_DIM = 32  # Dimension of embedding space\n",
        "SEQ_LEN = 1000  # Fixed sequence length\n",
        "VOCAB_SIZE = len(amino_acids)  # Assuming you have defined this\n",
        "\n",
        "# Updated model definition\n",
        "class CNNProtein(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(CNNProtein, self).__init__()\n",
        "        # Embedding layer to convert integer-encoded sequences to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(in_channels=embed_dim, out_channels=64, kernel_size=5, padding=2)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout probability 0.5\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * (SEQ_LEN // 4), 256)  # Output from CNN layer after pooling\n",
        "        self.fc2 = nn.Linear(256, 1)  # Binary output (cytoplasm or not)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the embedding layer\n",
        "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embed_dim)\n",
        "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, embed_dim, seq_len)\n",
        "\n",
        "        # Pass through convolutional layers\n",
        "        x = F.relu(self.conv1(x))  # Conv1d expects (batch_size, channels, seq_len)\n",
        "        x = self.pool(x)           # Pooling layer to reduce sequence length\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten the tensor\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))  # Sigmoid for binary classification\n",
        "        return x\n",
        "\n",
        "# Create the model\n",
        "model = CNNProtein(vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy for binary classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "c9btRvQJOHPQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "9JJuPv92wA3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0F1DgvWv57h",
        "outputId": "a63e107d-7b40-4e59-8a09-830c222f1df4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.6040, Val Loss: 0.5513\n",
            "Epoch 2/10, Train Loss: 0.5308, Val Loss: 0.5304\n",
            "Epoch 3/10, Train Loss: 0.4881, Val Loss: 0.5210\n",
            "Epoch 4/10, Train Loss: 0.4486, Val Loss: 0.5375\n",
            "Epoch 5/10, Train Loss: 0.4097, Val Loss: 0.5278\n",
            "Epoch 6/10, Train Loss: 0.3731, Val Loss: 0.5418\n",
            "Epoch 7/10, Train Loss: 0.3385, Val Loss: 0.5530\n",
            "Epoch 8/10, Train Loss: 0.3048, Val Loss: 0.6062\n",
            "Epoch 9/10, Train Loss: 0.2777, Val Loss: 0.6003\n",
            "Epoch 10/10, Train Loss: 0.2557, Val Loss: 0.5742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model begins to overfit after a few epochs, which is a common issue in neural networks, particularly when they lack sufficient regularization. Overfitting occurs when a model learns the noise and details of the training data too well, resulting in poor generalization to new, unseen data. This is evident when the training loss continues to decrease while the validation loss starts to increase.\n",
        "\n",
        "To mitigate this issue, techniques such as early stopping can be implemented. Early stopping involves monitoring the validation performance during training and saving the model checkpoint that exhibits the best validation performance, preventing the model from continuing to learn the noise of the training data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q0DAUgrE3EWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning ESM2-150M\n",
        "\n",
        "**Model**: **ESM2-150M (Evolutionary Scale Modeling)**\n",
        "\n",
        "Finally, we will fine-tune **ESM2-150M**, a transformer model specifically designed for protein sequences.  Fine-tuning involves adapting the pre-trained model to the specific classification problem by continuing the training process on the new dataset, adjusting its weights for the task at hand. ESM2 uses attention mechanisms to capture both local and global relationships in protein sequences, making it especially powerful for tasks like subcellular localization prediction.\n",
        "\n",
        "### Why ESM2?\n",
        "\n",
        "- **Global Context**: Transformers excel at capturing long-range dependencies in sequences, which is important for proteins where distant residues can interact to determine function and localization.\n",
        "- **Pre-training on Protein Data**: ESM2 has been pre-trained on massive protein datasets, making it particularly adept at understanding the language of proteins (i.e., amino acid sequences).\n",
        "- **State-of-the-Art Performance**: Transformer models like ESM2 are known to provide cutting-edge performance on a variety of protein prediction tasks, including structure and function prediction.\n",
        "\n",
        "**Key Components**:\n",
        "\n",
        "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different parts of the sequence, making it highly effective at capturing global patterns.\n",
        "- **Fine-tuning**: We can fine-tune ESM2 on the specific task of binary classification for cytoplasmic localization by updating the model weights on our labeled data."
      ],
      "metadata": {
        "id": "mlkDryg0OJD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import EsmForSequenceClassification, EsmTokenizer\n",
        "import torch.optim as optim\n",
        "# Load pre-trained ESM2 model and tokenizer\n",
        "model_name = \"facebook/esm2_t30_150M_UR50D\"\n",
        "tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
        "model = EsmForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "# Limit training samples for the sake of training time\n",
        "X_train_seq, X_val_seq, y_train_seq, y_val_seq = X_train_seq[:2000], X_val_seq, y_train[:2000], y_val\n",
        "\n",
        "# Tokenize input sequences for training and validation\n",
        "train_inputs = tokenizer(list(X_train_seq), return_tensors=\"pt\", padding=True, truncation=True, max_length=1000)\n",
        "val_inputs = tokenizer(list(X_val_seq), return_tensors=\"pt\", padding=True, truncation=True, max_length=1000)\n",
        "\n",
        "# Convert labels to tensors\n",
        "train_labels = torch.tensor(y_train_seq, dtype=torch.long)\n",
        "val_labels = torch.tensor(y_val_seq, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_data = TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)\n",
        "val_data = TensorDataset(val_inputs['input_ids'], val_inputs['attention_mask'], val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=2, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=2, shuffle=False)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Function to train the model\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    return avg_train_loss\n",
        "\n",
        "# Function to evaluate the model on validation set\n",
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct_preds = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            val_loss = outputs.loss\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct_preds += torch.sum(preds == labels).item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_accuracy = correct_preds / len(y_val)\n",
        "    return avg_val_loss, val_accuracy\n",
        "\n",
        "# Fine-tuning loop\n",
        "for epoch in range(3):  # Fine-tuning for 3 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, device)\n",
        "    val_loss, val_accuracy = validate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZM7LtWJOK4u",
        "outputId": "c6ac42f2-e722-43f3-cbd9-ccb2b329fee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ESM2 Advantages\n",
        "\n",
        "- **Pre-trained Knowledge**: The model benefits from knowledge gained during its pre-training phase, which was conducted on a massive protein dataset.\n",
        "- **Attention Mechanism**: ESM2 can capture long-range dependencies within protein sequences, which is critical for correctly predicting properties like localization.\n",
        "- **Drawbacks**: Large models like ESM2 are computationally expensive and require more resources for fine-tuning compared to simpler models like CNNs or logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In summary, we have explored various approaches to protein sequence analysis, from traditional machine learning methods to advanced deep learning techniques like Convolutional Neural Networks and the ESM2 model. These methods can provide significant insights into protein subcellular localization and other critical tasks. For further exploration of different models and challenges in protein understanding, I recommend visiting [TorchProtein's benchmark](https://torchprotein.ai/benchmark). This resource offers a comprehensive overview of existing models and their performance on a variety of protein-related tasks."
      ],
      "metadata": {
        "id": "BkWoFwRUOOcc"
      }
    }
  ]
}